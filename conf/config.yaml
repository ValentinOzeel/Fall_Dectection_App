YOLO:

  MODEL: yolov8n.pt #yolov8m.pt

  TRAIN_PARAMS:
    verbose: True
    workers: 1
    epochs: 175
    batch: 4
    imgsz: 640
    lr0: 0.00144615503351372
    lrf: 2.8465259340498403e-06
    optimizer: RAdam
    dropout: 0.10478331509780686
    patience: 50
    freeze: 7
 #   cls: 0.5
    momentum: 0.9087559754382691
    weight_decay: 0.00032459862370794976
  #  warmup_epochs: 1
    val: True
    plots: True
    seed: 0
    device: 0
    save_period: 0  # Save checkpoints every 5 epochs
    project: "runs\\detect\\training"  # Directory to save checkpoints

  VAL_PARAMS:
    imgsz: 640
    device: 0
      
  OPTUNA_PARAMS:  
      ###### Model's hyperparameter tuning
      ### Test different batch sizes to see which one maximizes GPU utilization and provides stable training.
  #  - ['batch', 'suggest_int', [2, 7]]
      ### Experiment with different image sizes to balance accuracy and computational efficiency.
  #  - ['imgsz', 'suggest_int', [500, 800]]
      ### Use a log-uniform distribution to explore a wide range of learning rates.
    - ['lr0', 'suggest_loguniform', [0.0012, 0.0025]]
 #     ### Use a log-uniform distribution to explore a wide range of final learning rates.
    - ['lrf', 'suggest_loguniform', [0.0000001, 0.00001]]
 #     ### Evaluate different optimizers to find the best one for your specific task and dataset.
 #   - ['optimizer', 'suggest_categorical', ['Adam', 'NAdam', 'RAdam', 'SGD']]    #['Adam', 'AdamW', 'NAdam', 'RAdam', 'RMSProp', 'SGD']
 #     ### Test different dropout rates to improve generalization and prevent overfitting.
    - ['dropout', 'suggest_uniform', [0.1, 0.5]]
 #     ### Determine the patience for early stopping to avoid unnecessary training once performance plateaus.
 #   - ['patience', 'suggest_int', [5, 10]]
 #     ### Adjust the number of frozen layers to benefit from pretrained weights while allowing fine-tuning.
 #   - ['freeze', 'suggest_int', [0, 10]]
 #     ### Weight of the box loss component in the loss function, influencing how much emphasis is placed on accurately predicting bounding box coordinates.
 ##   - ['box', 'suggest_uniform', [7.5, 15]]
 #     ### Weight of the classification loss in the total loss function, affecting the importance of correct class prediction relative to other components.
##    - ['cls', 'suggest_uniform', [0.5, 7.5]]
 #     ### Fine-tune momentum to help with faster convergence.
    - ['momentum', 'suggest_uniform', [0.90, 0.95]]
 #     ### Use log-uniform to penalize large weights and prevent overfitting.
    - ['weight_decay', 'suggest_loguniform', [0.000001, 0.001]]
 #     ### Adjust the warmup period to stabilize training in the initial epochs.
##    - ['warmup_epochs', 'suggest_int', [1, 10]]

  OPTUNA_FROZEN_PARAMS:
    verbose: False
    imgsz: 640
    epochs: 100
    patience: 20
    batch: 4
    workers: 1

    optimizer: RAdam
    freeze: 7

    val: True
    plots: True
    seed: 0
    device: 0
    save_period: 0  # Save checkpoints every 5 epochs
    project: "runs\\detect\\optuna"  # Directory to save checkpoints


  OPTUNA:
    trials: 10
